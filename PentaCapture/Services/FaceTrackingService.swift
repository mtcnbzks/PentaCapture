//
//  FaceTrackingService.swift
//  PentaCapture
//
//  Created by Mehmetcan BozkuÅŸ on 9.11.2025.
//

import ARKit
import Combine
import CoreImage
import UIKit
import simd

struct HeadPose: Equatable {
  let yaw: Double
  let pitch: Double
  let roll: Double
  let transform: simd_float4x4
  let position: simd_float3

  var yawDegrees: Double { yaw * 180.0 / .pi }
  var pitchDegrees: Double { pitch * 180.0 / .pi }
  var rollDegrees: Double { roll * 180.0 / .pi }

  /// Normalized center offset (0.0 = center)
  var centerOffset: CGPoint {
    CGPoint(x: CGFloat(position.x / 0.15), y: CGFloat(position.y / 0.2))
  }
}

enum FaceTrackingError: LocalizedError {
  case notSupported
  case sessionFailed
  case noFaceDetected

  var errorDescription: String? {
    switch self {
    case .notSupported: "Bu cihazda yÃ¼z takibi desteklenmiyor"
    case .sessionFailed: "ARSession baÅŸlatÄ±lamadÄ±"
    case .noFaceDetected: "YÃ¼z tespit edilemedi"
    }
  }

  var recoverySuggestion: String? {
    switch self {
    case .notSupported:
      "PentaCapture, iPhone X veya daha yeni bir cihaz gerektirir. LÃ¼tfen TrueDepth kamerasÄ± olan bir cihaz kullanÄ±n."
    case .sessionFailed:
      "UygulamayÄ± yeniden baÅŸlatmayÄ± deneyin. Sorun devam ederse lÃ¼tfen cihazÄ±nÄ±zÄ± yeniden baÅŸlatÄ±n."
    case .noFaceDetected:
      "YÃ¼zÃ¼nÃ¼zÃ¼n kamera gÃ¶rÃ¼ÅŸ alanÄ±nda olduÄŸundan ve ortamÄ±n yeterince aydÄ±nlÄ±k olduÄŸundan emin olun."
    }
  }
}

/// ARKit tabanlÄ± yÃ¼z takip servisi
@MainActor
class FaceTrackingService: NSObject, ObservableObject {
  @Published var isTracking = false
  @Published var currentHeadPose: HeadPose?
  @Published var error: FaceTrackingError?
  @Published var trackingState: String = "Not Started"

  nonisolated(unsafe) let isSupported: Bool
  let arSession = ARSession()  // Public - ARSCNView iÃ§in gerekli
  private var frameCount = 0

  // Idle timer management - auto-enable after 2 minutes
  private var idleTimerTask: Task<Void, Never>?

  override nonisolated init() {
    self.isSupported = ARFaceTrackingConfiguration.isSupported
    super.init()

    Task { @MainActor in
      print("ðŸŽ¯ FaceTrackingService initialized")
      print("   ARKit supported: \(self.isSupported)")
      print("   Device: \(UIDevice.current.model)")
    }
  }

  func startTracking() {
    guard isSupported else {
      print("âŒ ARKit not supported on this device")
      error = .notSupported
      return
    }

    guard !isTracking else {
      print("âš ï¸ Already tracking")
      // Make sure idle timer is disabled even if already tracking
      UIApplication.shared.isIdleTimerDisabled = true
      return
    }

    print("ðŸš€ Starting ARKit Face Tracking...")

    let configuration = ARFaceTrackingConfiguration()
    configuration.isLightEstimationEnabled = false
    configuration.maximumNumberOfTrackedFaces = 1
    // CRITICAL: Use .camera alignment for device-relative face orientation
    // This makes face angles independent of phone tilt (gravity)
    configuration.worldAlignment = .camera

    print("ðŸ“‹ Configuration:")
    print("   - worldAlignment: .camera (device-relative)")
    print("   - maxFaces: 1")

    arSession.delegate = self
    arSession.run(configuration, options: [.resetTracking, .removeExistingAnchors])

    isTracking = true
    error = nil
    frameCount = 0
    trackingState = "Starting..."

    // Disable idle timer to keep screen on during ARKit tracking
    UIApplication.shared.isIdleTimerDisabled = true
    print("ðŸ”† Screen idle timer disabled - screen will stay on")

    // Auto-enable idle timer after 2 minutes
    scheduleIdleTimerReenable()

    print("âœ… ARSession started")
  }

  func stopTracking() {
    guard isTracking else {
      // Re-enable idle timer even if not tracking
      UIApplication.shared.isIdleTimerDisabled = false
      return
    }

    print("â¹ï¸ Stopping ARKit Face Tracking...")
    arSession.pause()
    isTracking = false
    currentHeadPose = nil

    // Cancel any pending idle timer re-enable
    cancelIdleTimerReenable()
    // Re-enable idle timer to allow screen to sleep
    UIApplication.shared.isIdleTimerDisabled = false
    print("ðŸŒ™ Screen idle timer re-enabled - screen can sleep normally")
  }

  // MARK: - Idle Timer Management
  private func scheduleIdleTimerReenable() {
    // Cancel any existing task
    cancelIdleTimerReenable()

    print("â±ï¸ Scheduling idle timer re-enable in 2 minutes")
    idleTimerTask = Task { @MainActor in
      // Wait 2 minutes (120 seconds)
      try? await Task.sleep(nanoseconds: 120_000_000_000)

      // Check if task was cancelled
      guard !Task.isCancelled else {
        print("â±ï¸ Idle timer re-enable cancelled")
        return
      }

      // Re-enable idle timer after 2 minutes
      UIApplication.shared.isIdleTimerDisabled = false
      print("ðŸŒ™ Auto re-enabled idle timer after 2 minutes - screen can now sleep")
    }
  }

  private func cancelIdleTimerReenable() {
    idleTimerTask?.cancel()
    idleTimerTask = nil
  }

  // MARK: - High Resolution Capture (iOS 16+)

  /// Capture high-resolution photo directly from ARKit session
  /// This is the BEST approach: 0 latency, 0 race conditions, highest quality
  /// Per Apple WWDC 2022: Use captureHighResolutionFrame for still image capture
  @available(iOS 16.0, *)
  func captureHighResolutionPhoto() async throws -> UIImage {
    guard isSupported else {
      throw FaceTrackingError.notSupported
    }

    guard isTracking else {
      throw FaceTrackingError.sessionFailed
    }

    print("ðŸ“¸ [ARKit Capture] Requesting high-resolution frame...")
    let captureStartTime = Date()

    return try await withCheckedThrowingContinuation { continuation in
      arSession.captureHighResolutionFrame { [weak self] frame, error in
        let captureLatency = Date().timeIntervalSince(captureStartTime)
        print("ðŸ“¸ [ARKit Capture] Latency: \(String(format: "%.3f", captureLatency))s")

        if let error = error {
          let nsError = error as NSError

          // Handle specific ARKit capture errors
          if nsError.domain == "com.apple.arkit.error" {
            switch nsError.code {
            case 101:  // highResolutionFrameCaptureInProgress
              print("âŒ [ARKit Capture] Previous capture still in progress")
              continuation.resume(throwing: FaceTrackingError.sessionFailed)
              return
            case 102:  // highResolutionFrameCaptureFailed
              print("âŒ [ARKit Capture] Capture failed in pipeline")
              continuation.resume(throwing: FaceTrackingError.sessionFailed)
              return
            default:
              print("âŒ [ARKit Capture] Unknown error: \(error.localizedDescription)")
              continuation.resume(throwing: error)
              return
            }
          }

          print("âŒ [ARKit Capture] Error: \(error.localizedDescription)")
          continuation.resume(throwing: error)
          return
        }

        guard let frame = frame else {
          print("âŒ [ARKit Capture] No frame returned")
          continuation.resume(throwing: FaceTrackingError.noFaceDetected)
          return
        }

        // Extract high-resolution captured image
        let pixelBuffer = frame.capturedImage

        // Get buffer dimensions
        let width = CVPixelBufferGetWidth(pixelBuffer)
        let height = CVPixelBufferGetHeight(pixelBuffer)
        print("ðŸ“¸ [ARKit Capture] Captured frame: \(width)x\(height)")

        // Convert CVPixelBuffer to UIImage
        let processingStartTime = Date()
        guard let image = self?.convertPixelBufferToUIImage(pixelBuffer) else {
          print("âŒ [ARKit Capture] Failed to convert pixel buffer to UIImage")
          continuation.resume(throwing: FaceTrackingError.sessionFailed)
          return
        }

        let processingTime = Date().timeIntervalSince(processingStartTime)
        print("ðŸ“¸ [ARKit Capture] Image conversion: \(String(format: "%.3f", processingTime))s")
        print("ðŸ“¸ [ARKit Capture] Final image size: \(image.size)")

        // Mirror horizontally to match preview (front camera)
        let mirroredImage = self?.mirrorImageHorizontally(image) ?? image
        print("âœ… [ARKit Capture] High-res capture complete!")

        continuation.resume(returning: mirroredImage)
      }
    }
  }

  /// Check if high-resolution capture is available
  /// Per Apple docs: Requires iOS 16+ and active ARSession
  @available(iOS 16.0, *)
  var canCaptureHighResolution: Bool {
    return isSupported && isTracking
  }

  // MARK: - Image Conversion Helpers

  /// Convert CVPixelBuffer to UIImage with proper orientation
  /// Per Apple ARKit docs: "capturedImage pixel buffer is NOT adjusted for device orientation"
  /// ARKit always captures in landscape-right orientation, we need to rotate for portrait
  private func convertPixelBufferToUIImage(_ pixelBuffer: CVPixelBuffer) -> UIImage? {
    // Lock the pixel buffer for reading
    CVPixelBufferLockBaseAddress(pixelBuffer, .readOnly)
    defer { CVPixelBufferUnlockBaseAddress(pixelBuffer, .readOnly) }

    // Get buffer properties
    let width = CVPixelBufferGetWidth(pixelBuffer)
    let height = CVPixelBufferGetHeight(pixelBuffer)
    print("ðŸ“ [ARKit Image] CVPixelBuffer (raw camera): \(width)x\(height) (landscape)")

    // Create CIImage from pixel buffer
    let ciImage = CIImage(cvPixelBuffer: pixelBuffer)

    // Create CIContext for rendering (use Metal for better performance)
    let context = CIContext(options: [.useSoftwareRenderer: false])

    // Render to CGImage
    guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else {
      print("âŒ Failed to create CGImage from CIImage")
      return nil
    }

    // CRITICAL: ARKit camera orientation handling
    // Per Apple Documentation: "capturedImage is NOT adjusted for device orientation"
    // ARKit captures in landscape orientation from front camera
    // For portrait UI with front camera, we need .leftMirrored
    //
    // Orientation chart for front camera:
    // .up = 0Â° (no rotation) - wrong for portrait
    // .right = 90Â° CCW - was upside down
    // .rightMirrored = 90Â° CCW + mirror - was upside down
    // .left = 90Â° CW (landscape â†’ portrait correct direction)
    // .leftMirrored = 90Â° CW + mirror (CORRECT FOR FRONT CAMERA PORTRAIT)
    //
    // Why .leftMirrored?
    // - Front camera captures landscape-left naturally
    // - Need 90Â° CW rotation for portrait (.left)
    // - Need horizontal flip for mirror effect (Mirrored)

    print("ðŸ“ [ARKit Image] Applying .leftMirrored (portrait + mirror for front camera)")
    let image = UIImage(cgImage: cgImage, scale: 1.0, orientation: .leftMirrored)
    print("ðŸ“ [ARKit Image] Final UIImage: \(image.size), orientation: .leftMirrored")

    return image
  }

  /// Mirror image horizontally - NO LONGER NEEDED
  /// Orientation .rightMirrored already handles mirroring
  private func mirrorImageHorizontally(_ image: UIImage) -> UIImage {
    // With .rightMirrored orientation, additional mirroring is not needed
    // Just return as-is
    print("âœ… [ARKit Image] Skipping manual mirror (orientation already handles it)")
    return image
  }

  // Extract HeadPose from ARFaceAnchor
  // With worldAlignment = .camera, transform is already camera-relative
  nonisolated private func extractHeadPose(from faceAnchor: ARFaceAnchor) -> HeadPose {
    let eulerAngles = faceAnchor.transform.eulerAngles

    // Extract 3D position from transform matrix (column 3 = translation)
    let position = simd_float3(
      faceAnchor.transform.columns.3.x,
      faceAnchor.transform.columns.3.y,
      faceAnchor.transform.columns.3.z
    )

    // Axis mapping for front camera + .camera alignment:
    // - yaw (left/right turn) â†’ eulerAngles.x (negated for mirror)
    // - pitch (up/down tilt) â†’ eulerAngles.y
    // - roll (side-to-side tilt) â†’ eulerAngles.z
    return HeadPose(
      yaw: -Double(eulerAngles.x),
      pitch: Double(eulerAngles.y),
      roll: Double(eulerAngles.z),
      transform: faceAnchor.transform,
      position: position
    )
  }
}

// MARK: - ARSessionDelegate
extension FaceTrackingService: ARSessionDelegate {
  nonisolated func session(_ session: ARSession, didUpdate frame: ARFrame) {
    Task { @MainActor in
      self.frameCount += 1

      // Log ilk 10 frame ve sonra her 30 frame'de bir
      let shouldLog = self.frameCount <= 10 || self.frameCount % 30 == 0

      if shouldLog {
        print("ðŸ“¹ Frame #\(self.frameCount) - Anchors: \(frame.anchors.count)")
      }
    }

    guard let faceAnchor = frame.anchors.compactMap({ $0 as? ARFaceAnchor }).first else {
      Task { @MainActor in
        if self.frameCount <= 20 {
          print("âš ï¸ No face anchor in frame #\(self.frameCount)")
        }
        self.currentHeadPose = nil
        self.trackingState = "No Face Detected"
      }
      return
    }

    // With .camera worldAlignment, face is already in camera space
    // No need to pass camera transform - it's automatic!
    let headPose = extractHeadPose(from: faceAnchor)

    Task { @MainActor in
      self.currentHeadPose = headPose
      self.trackingState = faceAnchor.isTracked ? "Tracking" : "Not Tracked"

      // Ä°lk face bulunduÄŸunda log
      if self.frameCount <= 10 {
        print(
          "âœ… Face found! Yaw: \(String(format: "%.1fÂ°", headPose.yawDegrees)) Pitch: \(String(format: "%.1fÂ°", headPose.pitchDegrees)) Roll: \(String(format: "%.1fÂ°", headPose.rollDegrees))"
        )
      }
    }
  }

  nonisolated func session(_ session: ARSession, cameraDidChangeTrackingState camera: ARCamera) {
    Task { @MainActor in
      let stateDescription: String
      let reasonDescription: String

      switch camera.trackingState {
      case .normal:
        stateDescription = "Normal âœ…"
        reasonDescription = "Tracking is working properly"
      case .notAvailable:
        stateDescription = "Not Available âŒ"
        reasonDescription = "Tracking is not available"
      case .limited(let reason):
        stateDescription = "Limited âš ï¸"
        switch reason {
        case .initializing:
          reasonDescription = "Initializing..."
        case .relocalizing:
          reasonDescription = "Relocalizing..."
        case .excessiveMotion:
          reasonDescription = "Too much motion"
        case .insufficientFeatures:
          reasonDescription = "Not enough features"
        @unknown default:
          reasonDescription = "Unknown reason"
        }
      }

      self.trackingState = stateDescription
      print("ðŸ“Š Tracking State: \(stateDescription) - \(reasonDescription)")
    }
  }

  nonisolated func session(_ session: ARSession, didFailWithError error: Error) {
    print("âŒ ARSession failed: \(error.localizedDescription)")

    Task { @MainActor in
      self.error = .sessionFailed
      self.isTracking = false
      self.trackingState = "Failed"
    }
  }

  nonisolated func sessionWasInterrupted(_ session: ARSession) {
    print("â¸ï¸ ARSession interrupted")

    Task { @MainActor in
      self.isTracking = false
      self.trackingState = "Interrupted"
    }
  }

  nonisolated func sessionInterruptionEnded(_ session: ARSession) {
    print("â–¶ï¸ ARSession interruption ended")

    Task { @MainActor in
      if self.isSupported {
        self.startTracking()
      }
    }
  }
}
